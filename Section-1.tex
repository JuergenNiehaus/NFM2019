\section{Introduction}

The \emph{assured autonomy} program of DARPA has as its goal the development of rigorous design and analysis techniques for assuring the safety of \emph{learning-enabled cyber physical systems} operating in potentially adverse environments ~\cite{AssuredAutonomy}. This paper provides a preliminary contribution towards achieving these objectives, drawing on extensive discussions with automotive industry \cite{galbas,reco} in the formation of a large scale German research project on verification and validation techniques for HAVs. 

We present a methodology and architecture for assuring the safety of highly automated vehicles (HAV), which guarantees  that what the ego car believes to be true about its environment, and actual ground truth, rarely differ for all aspects which are relevant for ensuring the safety of the ego vehicle. How rare is rare enough is a matter of societal debates -- e.g. the German Department of Transportation requires HAVs to reduce the overall rate of fatalities. This paper answers the following question: if  r  is the level of societally accepted risks budgeted for misconceptions, how can we mathematically prove that perception of \enquote{relevant} environmental artefacts err at most with rate r? (We will clarify the vague term \enquote{relevant} in the text below)

We provide a mathematical setting for addressing this challenge, which is based on a reference architecture for the key functional ingredients of the perception chain. While clearly each OEM will have highly proprietary implementations, there seems to be an emerging consensus in ongoing and currently prepared projects on Verification and Validation of HAVs in Germany, that an agreement on a functional reference architecture is both desirable and achievable. Our proposal for the reference architecture uses labelled occupancy grids for fusion of data from radar, lidar, video, etc, and as interface to learning-algorithms based components for classifying objects in the environment of the ego-vehicle according to a (to be standardized) partially ordered ontology. We assume that each artifact in this ontology comes with class definitions characterizing both static and -- if applicable -- dynamic aspects, such as e.g. ODD dependent models for typical traffic behavior (e.g. characterizing variations in lateral and longitudinal acceleration of vehicles in a neighboring highway lane, or of pedestrians in an urban pedestrian crossing), and will exploit such knowledge in a prediction engine for evolving traffic. The reference architecture also requires for each sensor the capability to identify harsh environment conditions (where no sufficiently tight bounds for risk of misconceptions can be given), and exploits this information in mechanisms for sensor fusion. The reference architecture also gives formal meaning to the vague term \enquote{relevant} environmental artifact, in that the prediction engine provides feedback to the criticality of achieving high confidence information for individual fields in the occupancy grid: errors in misclassifications of objects only count, if they relate to such critical classifications. Finally, we propose a safety net reducing likelihood of misclassifications, in declaring \enquote{blindness} for classifications, where neither the evidence for existence of an artifact $a$ nor the evidence for absence of $a$ is sufficiently strong -- such declaration of \enquote{blindness}, if sustained over several cycles for relevant artifacts, will automatically induce minimal risk maneuvers (as does any detected usage of the HAV outside the allowed ODD). A key element of our approach is that we dynamically adjust thresholds for declaring blindness, thus allowing optimal trade-offs between availability and safety.

The main result of this paper is a methodology for formally establishing bounds on the risks of misperception for artifacts labelled \enquote{critical} by the maneuver layer. As in \cite{VitusTomlinCDC2013,galbas} we rely on reinforcement learning of models of traffic participants, represented as stochastic hybrid automata, within the prediction engine of our reference architecture. Whereas \cite{synthesisPerception} provides a formal synthesis based approach towards achieving safe controllers, our reference architecture does not restrict the typically highly proprietary planning and maneuver control of HAV, and instead provides a generic interface between such proprietary solutions and the perception chain allowing to tune the confidence level of individual percepts to their current criticality. This interface uses a version of their Chance Constrained Logic which allows to reason about classifications of artifacts in the occupancy grid. In contrast to \cite{SeshiaNFM17} our method uses on-line tuning of acceptance thresholds to increase confidence in individual percepts, thus allowing to optimize the tradeoff between availability (induced from "dont knows" and safety (bounding the error of misconception to societally accepted risk).

This paper is organized as follows: 
Section \ref{sec:mathmodel} defines a mathematical model of (ground truth) traffic flow in a given ODD, where observables are defined through the ontology and their associated classes. It then formalizes for a given ego car the partial knowledge the ego car acquires about its environment based on its perception chain, and refer to these as the ego car's beliefs about its environment. We then formally define the overarching safety requirement for HAVs in a given ODD, in relating the level of precision and risks of misconception between ground truth and beliefs of the ego vehicle. Section \ref{sec:refarchitecture} defines the reference architecture of the perception chain as outlined above.
Section \ref{sec:example} illustrates our proof methodology with a simple example. Section \ref{sec:proofrule} contains the formal proof rule and key elements of the underlying statistical arguments for bounding the level of risk. Section \ref{sec:relatedwork} discusses related work. We wrap up with a way forward in using this methodology.


