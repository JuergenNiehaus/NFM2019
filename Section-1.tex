\section{Introduction}

\del{The \emph{assured autonomy} program of DARPA has as its goal the development of rigorous design and analysis techniques for assuring the safety of \emph{learning-enabled cyber physical systems} operating in potentially adverse environments ~\cite{AssuredAutonomy}. This paper provides a preliminary contribution towards achieving these objectives, drawing on extensive discussions with automotive industry \cite{galbas,reco} in the formation of a large scale German research project on verification and validation techniques for HAVs. 
}
We present a methodology and architecture for assuring the safety of highly automated vehicles (HAV), which guarantees  that what the ego car believes to be true about its environment, and the actual ground truth, rarely differ for all aspects which are relevant for ensuring the safety of the ego vehicle. How rare is rare enough is a matter of societal debates -- e.g. the German Department of Transportation requires HAV to reduce the overall rate of fatalities. This paper answers the following question: if  r  is the level of societally accepted risk budgeted for misperceptions, how can we mathematically prove that perception of \enquote{relevant} environmental artifacts err with rate at most r? (We will clarify the vague term \enquote{relevant} in the text below)

We provide a mathematical setting for addressing this challenge, which is based on a reference architecture for the key functional ingredients of the perception chain. While each Original Equipment Manufacturer (OEM) has highly proprietary implementations, there seems to be an emerging consensus that an agreement on a functional reference architecture is both desirable and achievable. Our proposal for the reference architecture uses labelled occupancy grids for fusion of sensor data from radar, lidar, video, etc, and as interface to learning-algorithms based components for classifying objects in the environment of the ego-vehicle according to a partially ordered ontology. We assume that each artifact in this ontology comes with class definitions characterizing both static and -- if applicable -- dynamic aspects, such as ODD dependent models for typical traffic behavior (e.g. characterizing variations in lateral and longitudinal acceleration of vehicles in a neighboring highway lane, or of pedestrians in an urban pedestrian crossing), and will exploit such knowledge in a prediction engine for evolving traffic. The reference architecture also requires for each sensor the capability to identify harsh environment conditions (where no sufficiently tight bounds for risk of misperceptions can be given), and exploits this information in mechanisms for sensor fusion. The reference architecture also gives formal meaning to the vague term \enquote{relevant} environmental artifact: a prediction engine provides feedback regarding criticality of queries for individual fields in the occupancy grid: errors arising from misperceptions of objects only count, if they relate to such criticality. Finally, we propose a safety net reducing likelihood of misperceptions, in declaring \enquote{blindness} for perceptions, where neither the evidence for existence of an artifact $a$ nor the evidence for absence of $a$ is sufficiently strong -- such declaration of \enquote{blindness}, if sustained over several cycles for relevant artifacts, will automatically induce minimal risk maneuvers (as does any detected usage of the HAV outside the allowed ODD). A key element of our approach is that we dynamically adjust thresholds for declaring blindness, thus allowing optimal trade-offs between availability and safety.

The main result of this paper is a methodology for formally establishing bounds on the risks of misperception for critical artifacts. As in \cite{VitusTomlinCDC2013,galbas}, we rely on reinforcement learning of models of traffic participants, represented as (a class of) stochastic hybrid systems, within the prediction engine of our reference architecture. Whereas \cite{synthesisPerception}\cite{SeshiaNFM17} provide a formal synthesis
based approach towards achieving safe controllers, our reference architecture does not restrict the typically highly proprietary planning and maneuver control of HAV, and instead provides a generic interface between such proprietary solutions and the perception chain allowing to tune the confidence level of individual percepts to their current criticality, allowing to optimize the trade-off between availability (induced from "dont knows" and safety (bounding the error of misconception to societally accepted risk).

This paper is organized as follows: 
Section \ref{sec:mathmodel} defines a mathematical model of (ground truth) traffic flow in a given ODD, where observables are defined through the ontology and their associated classes. It then formalizes for a given ego car the partial knowledge the ego car acquires about its environment based on its perception chain, and refer to these as the ego car's beliefs about its environment. We then formally define the overarching safety requirement for HAV in a given ODD, in relating the level of precision and risks of misperception between ground truth and beliefs of the ego vehicle. Section \ref{sec:refarchitecture} defines the reference architecture of the perception chain as outlined above.
Section \ref{sec:example} illustrates our proof methodology with a simple example. Section \ref{sec:proofrule} contains the formal proof rule and key elements of the underlying statistical arguments for bounding the level of risk. Section \ref{sec:relatedwork} discusses related work. We wrap up with a way forward in using this methodology.


