\section{Introduction}
\textcolor{red}{The \}

Assuring the safety of HAV entails assuring, that what the ego car beliefs to be true about its environment, and actual ground truth, rarely differ for all aspects which are relevant for ensuring the safety of the ego vehicle. How rare is rare enough is a matter of societal debates -- e.g. the German Department of Transportations requires HAVs to reduce the overall rate of fatalities. This paper answers the following question: if  r  is the level of societally accepted risks budgeted for misconceptions, how can we mathematically prove that perception of \enquote{relevant} environmental artefacts err at most with rate r? (We will clarify the vague term \enquote{relevant} in the text below)

We provide a mathematical setting for addressing this challenge, which is based on a reference architecture for the key functional ingredients of the perception chain. While clearly each OEM will have highly proprietary implementations, there seems to an emerging consensus in ongoing and currently prepared projects on Verification and Validation of HAVs in Germany, that an agreement on a functional reference architecture is both desirable and achievable. Our proposal for the reference architecture uses labelled occupancy grids for fusion of data from radar, lidar, video, etc, and as interface to learning-algorithms based components for classifying objects in the environment of the ego-vehicle according to a (to be standardized) partially ordered ontology. We assume that each artefact in this ontology comes with class definitions characterizing both static and – if applicable – dynamic aspects, such as e.g. ODD dependent models for typical traffic behavior (e.g. characterizing variations in lateral and longitudinal acceleration of vehicles in a neighboring highway lane, or of pedestrians in an urban pedestrian crossing), and will exploit such knowledge to reduce misclassification rates. The reference architecture also requires for each sensor the capability to identify harsh environment conditions (where no sufficiently tight bounds for risk of misconceptions can be given), and exploits this information in mechanisms for sensor fusion. The reference architecture also gives formal meaning to the vague term \enquote{relevant} environmental artefact, in that the maneuver component provides feedback to the criticality of achieving high confidence information for individual fields in the occupancy grid: errors in misclassifications of objects only count, if they relate to such critical classifications. Finally, we propose a safety net reducing likelihood of misclassifications, in declaring \enquote{blindness} for classifications, where neither the evidence for existence of an artefact a nor the evidence for absence of a is sufficiently strong – such declaration of \enquote{blindness}, if sustained over several cycles for relevant artefacts, will automatically induce minimal risk maneuvers (as does any detected usage of the HAV outside the allowed ODD).

The main result of this paper is a methodology for formally establishing bounds on the risks of misperception for artefacts labelled \enquote{critical} by the maneuver layer. This proof is composed bottom up by propagating quality guarantees along the different levels of the perception chain. We document this proof through justifications, giving for each level the reasons why the risk of misconception can be bounded. Such justifications could provide a tool for post-mortem accident analysis.

This paper is organized as follows: 
Section \ref{sec:mathmodel} defines a mathematical model of (ground truth) traffic flow in a given ODD, where observables are defined through the ontology and their associated classes. It then formalizes for a given ego car the partial knowledge the ego car acquires about its environment based on its perception chain, and refer to these as the ego car's beliefs about its environment. We then formally define the overarching safety requirement for HAVs in a given ODD, in relating the level of precision and risks of misconception between ground truth and beliefs of the ego vehicle. Section \ref{sec:refarchitecture} defines the reference architecture of the perception chain as outlined above.
Section \ref{sec:example} illustrates our proof methodology with a simple example. Section \ref{sec:proofrule} contains the formal proof rule and key elements of the underlying statistical arguments for bounding the level of risk. Section \ref{sec:relatedwork} discusses related work. We wrap up with a way forward in using this methodology.


