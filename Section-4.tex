\section{A simple example}\label{sec:example}
In this section we develop a small  example illustrating that for taking a decision on the right maneuver the granularity of object classification varies depending on the goals to be achieved and also varies over time. Moreover, the example motivates why choosing the operation points of a learner dynamically depending on the situation allows to derive better perception guarantees, where operation points define when the output of a learner is interpreted as positive, negative or unknown classification result. 

Let us assume the ego car is at a situation as the blue car in Fig. \ref{fig:obstacle}.  
Further let ego's goals be (col-free) collision freedom and (em-brake) to avoid emergency maneuvers.
%
%For now let there be a (fused) sensor not in adverse conditions for each grid partition. 
For our scenario we assume % that the learning components are able to detect the road and lane separation line from the fused data of the occupancy grid.
%at times $t-\Delta$ to $t$. That is belief($t-\Delta$),\ldots, belief($t$) is guaranteed with sufficiently bounded  measurement tolerances and a sufficiently high confidence. 
%Moreover,
ego is able to identify two artifacts $A_1$ and $A_2$ (cf. Fig. \ref{fig:artifacts}) within its occupancy grid with high confidence. \\[2mm] 
%
\begin{minipage}{\textwidth}
	\begin{minipage}{0.45\textwidth}
	\centering
	\includegraphics[width=.8\textwidth]{ego+artifacts.eps}
	\captionof{figure}{Ego in scenario 1}\label{fig:artifacts}
\end{minipage}\hfill	
\begin{minipage}{0.45\textwidth}
	\centering
	\includegraphics[width=.8\textwidth]{ego+motorbike.eps}
	\captionof{figure}{Ego in scenario 2}\label{fig:mbike}
\end{minipage}
\vspace*{1mm}
\end{minipage}


Let the learning components have determined
%(based on the sensor measurements) 
 that $A_1$ is some vehicle and that it is not moving. 
Further the learner L(vehicle) classifies $A_2$  with high confidence as a vehicle. Let at time $t$ no other classifier notify to recognize the artifact --no resolution of percepts is hence required. Based on its beliefs that $A_2$ is a moving vehicle while $A_1$ is a non-moving obstacle, the ego car applies a general dynamics model for $A_2$ in its predictions of possible future evolutions. As ego cannot rule out that $A_2$ is a really fast car, it decides to stay at the lane. Emergency braking is not yet necessary. As time goes by and ego comes closer to both, $A_2$ and $A_1$, the classifier for tractor, L(tractor), is confident that  $A_2$ is actually tractor.
Since the previous belief of $A_2$ being a vehicle and the new belief of $A_2$ being a tractor is compatible, the resolution component derives that $A_2$ is believed to be a tractor. The prediction engine computes the possible future based on the belief that $A_1$ is a  non moving vehicle and $A_2$ is a tractor and determines that a circumvention maneuver is safe. Since the ego vehicle wants to avoid harsh braking (additionally to avoiding collisions), the maneuver selection determines a maneuver plan to circumvent $A_1$.

Note, that classifications of different detail are relevant for taking the decision on the maneuver. While it is sufficient to classify $A_1$ as a non-moving obstacle, the decision is highly sensitive to a more precise classification of $A_2$, the oncoming vehicle.
%
Let us now consider the influence of ego's goals on the choice of operation points of the involved classifiers. In our scenario ego prioritizes collision-avoidance over avoiding emergency brakings. Since the goal of avoiding collisions is safety critical, a very low probability (nearly zero) of violating (col-free) is acceptable. 
This implies that the (miss)perceptions/classifiers which may lead to a collision, have to be unlikely as well. Hence ego is interested in having a classification with a high true negative rate, thereby accepting that it performs an emergency braking maneuver also circumventing $A_1$ might be possible.
In order to achieve the less prioritized goal (em-brake) as often as possible, a high true positive rate of the classifier  L(tractor) is needed, because with a positive evidence that there is a tractor, ego can overtake. Since this goal is less important we assume that a higher probability of not satisfying this goal is accepted. In turn, the classifier is allowed more missclassifications wrt true positives.  
%
Moreover, note that since the currently active list of goals change over time (driving fast to an appoint, but after the appoint driving more relaxed), the requirements on the classifiers change over time as well.  

Now let us suppose we are in a situation as in Fig.~\ref{fig:mbike}. The choice whether ego circumvents $A_1$ now depends on $A_2$ and $A_3$. Ego will not choose to circumvent $A_1$ even when recognizing only one of the oncoming motorbikes and even when it only recognizes it as vehicle, in order to accomplish (col-free). So in this case the prediction engine determines an allowed rate of true positives of L(vehicle) regarding recognizing $A_2$ or $A_1$.
